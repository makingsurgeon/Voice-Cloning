{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8698def-e218-41dd-81f7-984262570ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a9ea8d-e2e3-4f52-befb-b9de8b3ac1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/Users/zihuiouyang/Downloads/LA/ASVspoof2019_LA_train/flac\"\n",
    "LABEL_FILE_PATH = \"/Users/zihuiouyang/Downloads/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "NUM_CLASSES = 2  # Number of classes (bonafide and spoof)\n",
    "SAMPLE_RATE = 16000  # Sample rate of your audio files\n",
    "DURATION = 5  # Duration of audio clips in seconds\n",
    "N_MELS = 128  # Number of Mel frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e18cc6f-8e5d-4e77-9621-fcf8679b9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c59f6c-4d82-4c66-a59c-06ebf6c1c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for filename in os.listdir('/Users/zihuiouyang/Downloads/cv-corpus-15.0-delta-2023-09-08/en/clips'):\n",
    "    if fnmatch.fnmatch(filename, '*.flac'):\n",
    "        a.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9817f1dd-837e-43f8-814b-83a7aa233c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a1 = random.sample(a,22800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83058a10-edd5-4dc7-a1d8-1f9781995724",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(len(a1)):\n",
    "    file_name = a1[i]\n",
    "    label = 1\n",
    "    labels[file_name] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bca06391-b23d-42e1-a1fc-df2a2d300110",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "max_time_steps = 250  # Define the maximum time steps for your model\n",
    "\n",
    "for file_name, label in labels.items():\n",
    "    file_path = os.path.join(\"/Users/zihuiouyang/Downloads/cv-corpus-15.0-delta-2023-09-08/en/clips\", file_name)\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < max_time_steps:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "    X.append(mel_spectrogram)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81618da6-5a58-4154-82e9-7971b812f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = {}\n",
    "with open(LABEL_FILE_PATH, 'r') as label_file:\n",
    "    lines = label_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    file_name = parts[1]\n",
    "    if parts[-1] == \"bonafide\":\n",
    "        continue\n",
    "    label = 0\n",
    "    labels1[file_name] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da26ac97-2008-4709-a9e9-3518754364ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_steps = 250  # Define the maximum time steps for your model\n",
    "\n",
    "for file_name, label in labels1.items():\n",
    "    file_path = os.path.join(DATASET_PATH, file_name + \".flac\")\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < max_time_steps:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "    X.append(mel_spectrogram)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "300c0d5b-7584-47e4-abdb-87c0232eba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y_encoded = to_categorical(y, NUM_CLASSES)\n",
    "split_index = int(0.8 * len(X))\n",
    "b = []\n",
    "for i in range(45600):\n",
    "    b.append(i)\n",
    "b1 = random.sample(b,36480)\n",
    "mask=np.full(len(b),False,dtype=bool)\n",
    "mask[b1]=True\n",
    "X_train, X_val = X[mask], X[~mask]\n",
    "y_train, y_val = y_encoded[mask], y_encoded[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e98e6dd-2fd1-4d1b-85f7-995ae18bedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (N_MELS, X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a51d58ce-ed18-4fc8-87ed-34f729d37fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128)               194048    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 48)                3120      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 98        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222034 (867.32 KB)\n",
      "Trainable params: 222034 (867.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(LSTM(128,input_shape=input_shape))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "430b485e-c239-4085-8b20-cbd4663f824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f14d68db-f8cf-4e55-bf62-c6cf8ed076be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 10:03:33.706971: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140/1140 [==============================] - 89s 72ms/step - loss: 0.6382 - accuracy: 0.5639 - val_loss: 6.1694 - val_accuracy: 0.4902\n",
      "Epoch 2/10\n",
      "1140/1140 [==============================] - 80s 70ms/step - loss: 0.6373 - accuracy: 0.5664 - val_loss: 6.3593 - val_accuracy: 0.4902\n",
      "Epoch 3/10\n",
      "1140/1140 [==============================] - 82s 72ms/step - loss: 0.7044 - accuracy: 0.5550 - val_loss: 10.2207 - val_accuracy: 0.4902\n",
      "Epoch 4/10\n",
      "1140/1140 [==============================] - 81s 71ms/step - loss: 0.6592 - accuracy: 0.5490 - val_loss: 9.8645 - val_accuracy: 0.4902\n",
      "Epoch 5/10\n",
      "1140/1140 [==============================] - 80s 70ms/step - loss: 0.6535 - accuracy: 0.5542 - val_loss: 10.1938 - val_accuracy: 0.4902\n",
      "Epoch 6/10\n",
      "1140/1140 [==============================] - 82s 72ms/step - loss: 0.6524 - accuracy: 0.5543 - val_loss: 9.7608 - val_accuracy: 0.4902\n",
      "Epoch 7/10\n",
      "1140/1140 [==============================] - 82s 72ms/step - loss: 0.6530 - accuracy: 0.5508 - val_loss: 9.8448 - val_accuracy: 0.4902\n",
      "Epoch 8/10\n",
      "1140/1140 [==============================] - 83s 73ms/step - loss: 0.6520 - accuracy: 0.5562 - val_loss: 8.6342 - val_accuracy: 0.4902\n",
      "Epoch 9/10\n",
      "1140/1140 [==============================] - 81s 71ms/step - loss: 0.6538 - accuracy: 0.5559 - val_loss: 9.7138 - val_accuracy: 0.4902\n",
      "Epoch 10/10\n",
      "1140/1140 [==============================] - 83s 73ms/step - loss: 0.6518 - accuracy: 0.5516 - val_loss: 9.9296 - val_accuracy: 0.4902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28e5d4100>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7f91231-338d-45b5-af70-1af46cc66aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihuiouyang/anaconda3/envs/coqui/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/Users/zihuiouyang/Documents/audio_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e905d696-760f-4478-a201-2a849e18f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET_PATH = \"./TestEvaluation\"\n",
    "MODEL_PATH = \"/Users/zihuiouyang/Documents/audio_classifier.h5\"  # Replace with the actual path to your saved model\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5\n",
    "N_MELS = 128\n",
    "MAX_TIME_STEPS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2113f554-9aab-4e04-8e2b-821c73602162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "287c8126-e76b-4bce-91fd-89a8fad66f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.9054447e-09, 1.0000000e+00],\n",
       "       [7.7551896e-08, 9.9999988e-01],\n",
       "       [2.4914377e-09, 1.0000000e+00],\n",
       "       [2.2560271e-09, 1.0000000e+00],\n",
       "       [1.9114728e-09, 1.0000000e+00],\n",
       "       [2.4700697e-09, 1.0000000e+00],\n",
       "       [1.9063242e-09, 1.0000000e+00],\n",
       "       [3.8757135e-09, 1.0000000e+00],\n",
       "       [2.5543032e-09, 1.0000000e+00],\n",
       "       [2.0504369e-09, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = []\n",
    "\n",
    "test_files = os.listdir(TEST_DATASET_PATH)\n",
    "for file_name in test_files:\n",
    "    file_path = os.path.join(TEST_DATASET_PATH, file_name)\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < MAX_TIME_STEPS:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, MAX_TIME_STEPS - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :MAX_TIME_STEPS]\n",
    "\n",
    "    X_test.append(mel_spectrogram)\n",
    "\n",
    "# Convert list to numpy array\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Predict using the loaded model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01489bc-3eee-4224-a112-ba6fde1cf12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
